# LlamaNet

LlamaNet is a decentralized inference swarm for LLM models using llama.cpp. It allows you to distribute inference across multiple nodes and automatically routes requests to the most available nodes.

## Features

- Registry service for node discovery and load balancing
- Inference nodes that serve LLM models using llama.cpp
- Client library for easy integration
- Automatic node selection based on load and performance
- Docker support for easy deployment

## Requirements

- Python 3.8+
- LLM models in GGUF format
- Docker (optional, for containerized deployment)

## Installation

### From Source

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/llamanet.git
   cd llamanet
   ```

2. Install the package:
   ```bash
   pip install -e .
   ```

### Using pip

```bash
pip install llamanet
```

## Running Locally

### 1. Start the Registry Service

The registry keeps track of all available inference nodes.

```bash
# Set configuration (optional)
export REGISTRY_PORT=8080

# Start the registry
python -m registry.server
```

### 2. Start Inference Node(s)

You can start multiple inference nodes, each with a different model.

```bash
# Set configuration
export MODEL_PATH=/path/to/your/model.gguf
export PORT=8000  # Use different ports for multiple nodes
export REGISTRY_URL=http://localhost:8080

# Start the inference node
python -m inference_node.server
```

### 3. Use the Client

```python
from client.api import Client

# Create a client
client = Client(
    registry_url="http://localhost:8080",
    model="your-model-name"  # Optional: specify model
)

# Generate text
response = client.generate(
    prompt="What is LlamaNet?",
    max_tokens=150,
    temperature=0.7
)

if response:
    print(f"Generated by node {response.node_id}:")
    print(response.text)
    print(f"Generated {response.tokens_generated} tokens in {response.generation_time:.2f} seconds")
```

## Docker Deployment

### Using docker-compose

1. Place your model files in the `models` directory

2. Start the services:
   ```bash
   cd docker
   docker-compose up -d
   ```

### Manual Docker Setup

1. Build and start the registry:
   ```bash
   docker build -f docker/registry.Dockerfile -t llamanet-registry .
   docker run -d -p 8080:8080 --name llamanet-registry llamanet-registry
   ```

2. Build and start an inference node:
   ```bash
   docker build -f docker/inference.Dockerfile -t llamanet-inference .
   docker run -d -p 8000:8000 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e REGISTRY_URL=http://host-ip:8080 \
     --name llamanet-inference llamanet-inference
   ```

## Configuration

### Registry Service

| Environment Variable | Default | Description |
|----------------------|---------|-------------|
| REGISTRY_HOST        | 0.0.0.0 | Host to bind the registry service |
| REGISTRY_PORT        | 8080    | Port for the registry service |
| NODE_TTL             | 30      | Time (in seconds) after which a node is considered offline |

### Inference Node

| Environment Variable | Default | Description |
|----------------------|---------|-------------|
| MODEL_PATH           | -       | Path to the GGUF model file |
| HOST                 | 0.0.0.0 | Host to bind the inference service |
| PORT                 | 8000    | Port for the inference service |
| REGISTRY_URL         | http://localhost:8080 | URL of the registry service |
| HEARTBEAT_INTERVAL   | 10      | Interval (in seconds) for sending heartbeats |
| N_CTX                | 2048    | Context size for the model |
| N_BATCH              | 8       | Batch size for inference |
| N_GPU_LAYERS         | 0       | Number of layers to offload to GPU (0 = CPU only) |

## License

MIT
