# LlamaNet

LlamaNet is a decentralized inference swarm for LLM models using llama.cpp. It uses Kademlia DHT for truly distributed node discovery without any central registry.

## Features

- **Decentralized DHT-based node discovery** using Kademlia protocol
- Inference nodes that serve LLM models using llama.cpp
- **Async Client Library** for easy integration with async/await support
- Automatic node selection based on load and performance
- No single point of failure - fully distributed architecture
- Docker support for easy deployment

## Requirements

- Python 3.8+
- LLM models in GGUF format
- Docker (optional, for containerized deployment)

## Installation

### From Source

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/llamanet.git
   cd llamanet
   ```

2. Install the package:
   ```bash
   pip install --editable . --use-pep517
   ```

### Using pip

```bash
pip install llamanet
```

## Running Locally

### 1. Download a Model

First, download a GGUF model file. For example:
```bash
# Create models directory
mkdir -p models

# Download a GGUF model (example - replace with actual GGUF model URL)
# wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf -O models/model.gguf
# For now, place your .gguf model file in the models directory
```

### 2. Start Bootstrap Node

The first inference node acts as a bootstrap node for the DHT network.

**Using Command Line Arguments:**
```bash
python -m inference_node.server \
  --model-path ./models/model.gguf \
  --port 8000 \
  --dht-port 8001 \
  --node-id bootstrap-node
```

**Using Environment Variables:**
```bash
export MODEL_PATH=./models/model.gguf
export PORT=8000
export DHT_PORT=8001
export NODE_ID=bootstrap-node
export BOOTSTRAP_NODES=""

python -m inference_node.server
```

### 3. Start Additional Inference Nodes

**Using Command Line Arguments:**
```bash
python -m inference_node.server \
  --model-path ./models/model.gguf \
  --port 8002 \
  --dht-port 8003 \
  --bootstrap-nodes localhost:8001
```

**Using Environment Variables:**
```bash
export MODEL_PATH=./models/model.gguf
export PORT=8002
export DHT_PORT=8003
export BOOTSTRAP_NODES="localhost:8001"

python -m inference_node.server
```

### 4. Use the Client

```python
import asyncio
from client.api import Client

async def main():
    client = Client(
        bootstrap_nodes="localhost:8001",
        model="model"  # Use the model name (filename without extension)
    )
    
    try:
        response = await client.generate(
            prompt="What is LlamaNet?",
            max_tokens=150,
            temperature=0.7
        )
        
        if response:
            print(f"Generated by node {response.node_id}:")
            print(response.text)
        else:
            print("Failed to generate text")
    finally:
        await client.close()

if __name__ == "__main__":
    asyncio.run(main())
```

## Quick Start Commands

```bash
# 1. Install dependencies
pip install -e .

# 2. Download a model (example)
mkdir models
# Place your .gguf model file in the models directory

# 3. Start bootstrap node
python -m inference_node.server --model-path ./models/your-model.gguf

# 4. In another terminal, start additional node
python -m inference_node.server \
  --model-path ./models/your-model.gguf \
  --port 8002 \
  --dht-port 8003 \
  --bootstrap-nodes localhost:8001

# 5. Use the client (see examples/simple_client.py for full example)
# Note: client.generate() must be awaited in an async function
```

## Docker Deployment

### Using docker-compose

1. Place your model files in the `models` directory

2. Start the services:
   ```bash
   cd docker
   docker-compose up -d
   ```

This will start:
- Bootstrap node on ports 8000 (HTTP) and 8001 (DHT)
- Additional inference nodes that automatically join the DHT network

### Manual Docker Setup

1. Build the inference image:
   ```bash
   docker build -f docker/inference.Dockerfile -t llamanet-inference .
   ```

2. Start bootstrap node:
   ```bash
   docker run -d -p 8000:8000 -p 8001:8001 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e PORT=8000 \
     -e DHT_PORT=8001 \
     -e NODE_ID=bootstrap-node \
     -e BOOTSTRAP_NODES="" \
     --name llamanet-bootstrap llamanet-inference
   ```

3. Start additional nodes:
   ```bash
   docker run -d -p 8002:8000 -p 8003:8001 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e PORT=8000 \
     -e DHT_PORT=8001 \
     -e BOOTSTRAP_NODES="host-ip:8001" \
     --name llamanet-node1 llamanet-inference
   ```

## Configuration

### Inference Node

| Environment Variable | Default | Description |
|----------------------|---------|-------------|
| MODEL_PATH           | -       | Path to the GGUF model file |
| HOST                 | 0.0.0.0 | Host to bind the inference service |
| PORT                 | 8000    | Port for the inference HTTP API |
| DHT_PORT             | 8001    | Port for Kademlia DHT protocol |
| NODE_ID              | auto    | Unique identifier for this node |
| BOOTSTRAP_NODES      | ""      | Comma-separated list of bootstrap nodes (ip:port) |
| HEARTBEAT_INTERVAL   | 10      | Interval (in seconds) for DHT updates |
| N_CTX                | 2048    | Context size for the model |
| N_BATCH              | 8       | Batch size for inference |
| N_GPU_LAYERS         | 0       | Number of layers to offload to GPU (0 = CPU only) |

### Client Configuration

```python
client = Client(
    bootstrap_nodes="node1:8001,node2:8001",  # Bootstrap DHT nodes
    model="model-name",                       # Filter by model (optional)
    min_tps=1.0,                             # Minimum tokens per second
    max_load=0.8,                            # Maximum load threshold
    dht_port=8001                            # DHT port for client
)

# Note: All client operations are async and must be awaited
response = await client.generate("Your prompt here")
await client.close()  # Always close the client when done
```

## Architecture

LlamaNet uses a **Kademlia Distributed Hash Table (DHT)** for node discovery:

1. **No Central Registry**: Nodes discover each other through the DHT network
2. **Bootstrap Process**: First node creates the network, others join via bootstrap nodes
3. **Automatic Discovery**: Clients query the DHT to find available inference nodes
4. **Fault Tolerance**: Network continues operating even if nodes leave
5. **Scalability**: Logarithmic lookup time O(log n) for node discovery

## System Architecture Diagrams

### 1. Network Formation Flow

```mermaid
graph TD
    A[Bootstrap Node] -->|Starts DHT| B[DHT Network]
    C[Node 1] -->|Joins via Bootstrap| B
    D[Node 2] -->|Joins via Bootstrap| B
    E[Node 3] -->|Joins via Node 1| B
    
    B --> F[Distributed Hash Table]
    F --> G[Key: model:llama-7b]
    F --> H[Key: node:abc123...]
    F --> I[Key: all_nodes]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style F fill:#fff3e0
```

### 2. Node Discovery Process

```mermaid
sequenceDiagram
    participant C as Client
    participant DHT as DHT Network
    participant N1 as Node 1
    participant N2 as Node 2
    participant N3 as Node 3

    C->>DHT: Query "model:llama-7b"
    DHT->>C: Return [Node1, Node2, Node3]
    
    C->>N1: Check load/health
    N1->>C: Load: 0.3, TPS: 15.2
    
    C->>N2: Check load/health
    N2->>C: Load: 0.7, TPS: 12.1
    
    C->>N3: Check load/health
    N3->>C: Load: 0.1, TPS: 18.5
    
    Note over C: Select Node 3 (lowest load)
    C->>N3: Send inference request
    N3->>C: Return generated text
```

### 3. DHT Key Distribution

```
DHT Storage Keys:
┌─────────────────────────────────────────────────────────────┐
│ Key: "model:llama-7b"                                       │
│ Value: [                                                    │
│   {node_id: "abc123", ip: "192.168.1.10", port: 8000},    │
│   {node_id: "def456", ip: "192.168.1.11", port: 8000},    │
│   {node_id: "ghi789", ip: "192.168.1.12", port: 8000}     │
│ ]                                                           │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│ Key: "node:abc123"                                          │
│ Value: {                                                    │
│   node_id: "abc123", ip: "192.168.1.10", port: 8000,     │
│   model: "llama-7b", load: 0.3, tps: 15.2, uptime: 3600  │
│ }                                                           │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│ Key: "all_nodes"                                            │
│ Value: [All active nodes regardless of model]              │
└─────────────────────────────────────────────────────────────┘
```

### 4. Client Request Flow

```mermaid
graph LR
    A[Client Request] --> B{Select API Mode}
    B -->|LlamaNet| C[Native API]
    B -->|OpenAI| D[OpenAI Compatible]
    
    C --> E[DHT Discovery]
    D --> E
    
    E --> F[Node Selection]
    F --> G{Load Balancing}
    G -->|Lowest Load| H[Selected Node]
    G -->|Failover| I[Backup Node]
    
    H --> J[HTTP Request]
    I --> J
    J --> K[LLM Inference]
    K --> L[Response]
    
    style A fill:#e8f5e8
    style H fill:#fff2cc
    style K fill:#ffe6cc
    style L fill:#e1f5fe
```

### 5. Network Topology Example

```
                    Internet/Local Network
                           │
        ┌──────────────────┼──────────────────┐
        │                  │                  │
   ┌────▼────┐        ┌────▼────┐        ┌────▼────┐
   │ Node A  │◄──────►│ Node B  │◄──────►│ Node C  │
   │ :8000   │   DHT  │ :8002   │   DHT  │ :8004   │
   │ :8001   │        │ :8003   │        │ :8005   │
   └─────────┘        └─────────┘        └─────────┘
        ▲                  ▲                  ▲
        │ HTTP             │ HTTP             │ HTTP
        │                  │                  │
   ┌────▼────┐        ┌────▼────┐        ┌────▼────┐
   │Client 1 │        │Client 2 │        │Web UI   │
   │         │        │         │        │         │
   └─────────┘        └─────────┘        └─────────┘

Legend:
- HTTP Ports: 8000, 8002, 8004 (Inference API)
- DHT Ports:  8001, 8003, 8005 (Node Discovery)
- DHT: Kademlia protocol connections
```

### 6. OpenAI API Compatibility Layer

```mermaid
graph TD
    A[OpenAI Client] --> B[/v1/chat/completions]
    A --> C[/v1/completions]
    A --> D[/v1/models]
    
    B --> E[Message Conversion]
    C --> F[Prompt Processing]
    D --> G[Model Listing]
    
    E --> H[LlamaNet Core]
    F --> H
    G --> H
    
    H --> I[DHT Discovery]
    I --> J[Node Selection]
    J --> K[llama.cpp Inference]
    K --> L[Response Formatting]
    
    L --> M[OpenAI Format]
    M --> A
    
    style A fill:#e3f2fd
    style H fill:#f3e5f5
    style K fill:#fff3e0
    style M fill:#e8f5e8
```

### 7. Web UI Architecture

```mermaid
graph TB
    A[Web Browser] --> B[Static Files]
    B --> C[Bootstrap CSS]
    B --> D[Custom CSS]
    B --> E[JavaScript App]
    
    E --> F[Network Monitor]
    E --> G[Chat Interface]
    E --> H[API Selector]
    
    F --> I[/dht/status]
    G --> J{API Mode}
    J -->|LlamaNet| K[/generate]
    J -->|OpenAI| L[/v1/chat/completions]
    
    I --> M[DHT Network Info]
    K --> N[Native Response]
    L --> O[OpenAI Response]
    
    N --> P[Markdown Rendering]
    O --> P
    P --> Q[Chat Display]
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style P fill:#fff3e0
    style Q fill:#e8f5e8
```

### 8. Data Flow Summary

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Client    │───►│ DHT Network │───►│ Node Select │───►│ Inference   │
│             │    │             │    │             │    │             │
│ • Web UI    │    │ • Discovery │    │ • Load Bal. │    │ • llama.cpp │
│ • API Call  │    │ • Routing   │    │ • Failover  │    │ • Generate  │
│ • OpenAI    │    │ • Storage   │    │ • Health    │    │ • Response  │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘
       ▲                                                         │
       │                                                         │
       └─────────────────────────────────────────────────────────┘
                           Response Flow
```

### Network Formation

```
Bootstrap Node (8001) ← Node 1 (8003) ← Node 2 (8005)
       ↑                     ↑              ↑
   Client connects      Joins DHT      Joins DHT
```

### DHT Keys

- `model:{model_name}` - Find nodes serving specific models
- `node:{node_id}` - Find specific nodes by ID  
- `all_nodes` - Discover any available nodes

## Component Interaction Flow

### 1. Node Startup Sequence
1. **Load Configuration** - Parse CLI args and environment variables
2. **Initialize LLM** - Load GGUF model with llama.cpp
3. **Start DHT Node** - Create Kademlia node on available port
4. **Join Network** - Connect to bootstrap nodes if specified
5. **Start HTTP Server** - Serve inference API and web UI
6. **Begin Publishing** - Announce availability to DHT every 10 seconds

### 2. Client Discovery Process
1. **Create DHT Client** - Initialize Kademlia client
2. **Query Network** - Search for nodes by model or all nodes
3. **Health Check** - Verify node availability and performance
4. **Load Balancing** - Select optimal node based on load/TPS
5. **Send Request** - Make HTTP call to selected node
6. **Handle Response** - Process result or failover to backup node

### 3. Request Processing Pipeline
1. **Receive Request** - HTTP endpoint receives generation request
2. **Validate Input** - Check prompt, parameters, and format
3. **Queue Processing** - Add to inference queue if needed
4. **LLM Generation** - Call llama.cpp with specified parameters
5. **Format Response** - Convert to LlamaNet or OpenAI format
6. **Update Metrics** - Track tokens, timing, and load statistics
7. **Return Result** - Send formatted response to client

This architecture ensures **high availability**, **automatic scaling**, and **fault tolerance** while maintaining **compatibility** with existing OpenAI-based applications.

## Troubleshooting

### No Nodes Available
- Ensure bootstrap nodes are running and accessible
- Check DHT_PORT is not blocked by firewall
- Verify BOOTSTRAP_NODES environment variable is set correctly

### Connection Issues
- Check that inference nodes are publishing to DHT (logs should show "Published node info")
- Verify client can reach bootstrap nodes on DHT port
- Ensure model names match between nodes and client requests

## OpenAI-Compatible API

LlamaNet now supports OpenAI-compatible endpoints, making it a drop-in replacement for OpenAI's API in many applications.

### Supported Endpoints

- `GET /v1/models` - List available models
- `POST /v1/completions` - Text completion (compatible with OpenAI's completions API)
- `POST /v1/chat/completions` - Chat completion (compatible with OpenAI's chat API)

### Using with OpenAI Python Library

```python
import openai

# Configure to use LlamaNet
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy-key"  # Not used but required by the library

# Text completion
response = openai.Completion.create(
    model="llamanet",
    prompt="What is artificial intelligence?",
    max_tokens=100,
    temperature=0.7
)
print(response.choices[0].text)

# Chat completion
response = openai.ChatCompletion.create(
    model="llamanet",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing."}
    ],
    max_tokens=150
)
print(response.choices[0].message.content)
```

### Using with curl

```bash
# List models
curl http://localhost:8000/v1/models

# Text completion
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llamanet",
    "prompt": "What is machine learning?",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# Chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llamanet",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "max_tokens": 100
  }'
```

### Compatibility Notes

- The API is compatible with most OpenAI client libraries
- Some advanced features (like function calling) are not yet supported
- Token counting is approximate and may differ from OpenAI's implementation
- Streaming responses are not yet implemented but planned for future releases

---

## Made with ❤️ using MACH-AI

This project was built with love using [MACH-AI](https://machaao.com), an AI-powered development platform that enables rapid creation of production-scale applications. MACH-AI helped accelerate the development of LlamaNet's distributed architecture, OpenAI-compatible APIs, and web interface.

**Key features developed with MACH-AI:**
- 🌐 Decentralized DHT-based node discovery
- 🤖 OpenAI-compatible API endpoints
- 💻 Interactive web UI with real-time updates
- 🔄 Real-time network monitoring
- 📊 Performance metrics and health checks

Learn more about building AI-powered applications at [machaao.com](https://machaao.com)

## License

MIT
