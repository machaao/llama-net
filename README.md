# LlamaNet

LlamaNet is a decentralized inference swarm for LLM models using llama.cpp. It uses Kademlia DHT for truly distributed node discovery without any central registry.

## Features

- **Decentralized DHT-based node discovery** using Kademlia protocol
- Inference nodes that serve LLM models using llama.cpp
- Client library for easy integration
- Automatic node selection based on load and performance
- No single point of failure - fully distributed architecture
- Docker support for easy deployment

## Requirements

- Python 3.8+
- LLM models in GGUF format
- Docker (optional, for containerized deployment)

## Installation

### From Source

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/llamanet.git
   cd llamanet
   ```

2. Install the package:
   ```bash
   pip install --editable . --use-pep517
   ```

### Using pip

```bash
pip install llamanet
```

## Running Locally

### 1. Download a Model

First, download a GGUF model file. For example:
```bash
# Create models directory
mkdir -p models

# Download a small model (replace with your preferred model)
wget https://huggingface.co/microsoft/DialoGPT-medium/resolve/main/pytorch_model.bin -O models/model.gguf
```

### 2. Start Bootstrap Node

The first inference node acts as a bootstrap node for the DHT network.

**Using Command Line Arguments:**
```bash
python -m inference_node.server \
  --model-path ./models/model.gguf \
  --port 8000 \
  --dht-port 8001 \
  --node-id bootstrap-node
```

**Using Environment Variables:**
```bash
export MODEL_PATH=./models/model.gguf
export PORT=8000
export DHT_PORT=8001
export NODE_ID=bootstrap-node
export BOOTSTRAP_NODES=""

python -m inference_node.server
```

### 3. Start Additional Inference Nodes

**Using Command Line Arguments:**
```bash
python -m inference_node.server \
  --model-path ./models/model.gguf \
  --port 8002 \
  --dht-port 8003 \
  --bootstrap-nodes localhost:8001
```

**Using Environment Variables:**
```bash
export MODEL_PATH=./models/model.gguf
export PORT=8002
export DHT_PORT=8003
export BOOTSTRAP_NODES="localhost:8001"

python -m inference_node.server
```

### 4. Use the Client

```python
import asyncio
from client.api import Client

async def main():
    client = Client(
        bootstrap_nodes="localhost:8001",
        model="model"  # Use the model name (filename without extension)
    )
    
    try:
        response = client.generate(
            prompt="What is LlamaNet?",
            max_tokens=150,
            temperature=0.7
        )
        
        if response:
            print(f"Generated by node {response.node_id}:")
            print(response.text)
        else:
            print("Failed to generate text")
    finally:
        await client.close()

if __name__ == "__main__":
    asyncio.run(main())
```

## Quick Start Commands

```bash
# 1. Install dependencies
pip install -e .

# 2. Download a model (example)
mkdir models
# Place your .gguf model file in the models directory

# 3. Start bootstrap node
python -m inference_node.server --model-path ./models/your-model.gguf

# 4. In another terminal, start additional node
python -m inference_node.server \
  --model-path ./models/your-model.gguf \
  --port 8002 \
  --dht-port 8003 \
  --bootstrap-nodes localhost:8001
```

## Docker Deployment

### Using docker-compose

1. Place your model files in the `models` directory

2. Start the services:
   ```bash
   cd docker
   docker-compose up -d
   ```

This will start:
- Bootstrap node on ports 8000 (HTTP) and 8001 (DHT)
- Additional inference nodes that automatically join the DHT network

### Manual Docker Setup

1. Build the inference image:
   ```bash
   docker build -f docker/inference.Dockerfile -t llamanet-inference .
   ```

2. Start bootstrap node:
   ```bash
   docker run -d -p 8000:8000 -p 8001:8001 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e PORT=8000 \
     -e DHT_PORT=8001 \
     -e NODE_ID=bootstrap-node \
     -e BOOTSTRAP_NODES="" \
     --name llamanet-bootstrap llamanet-inference
   ```

3. Start additional nodes:
   ```bash
   docker run -d -p 8002:8000 -p 8003:8001 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e PORT=8000 \
     -e DHT_PORT=8001 \
     -e BOOTSTRAP_NODES="host-ip:8001" \
     --name llamanet-node1 llamanet-inference
   ```

## Configuration

### Inference Node

| Environment Variable | Default | Description |
|----------------------|---------|-------------|
| MODEL_PATH           | -       | Path to the GGUF model file |
| HOST                 | 0.0.0.0 | Host to bind the inference service |
| PORT                 | 8000    | Port for the inference HTTP API |
| DHT_PORT             | 8001    | Port for Kademlia DHT protocol |
| NODE_ID              | auto    | Unique identifier for this node |
| BOOTSTRAP_NODES      | ""      | Comma-separated list of bootstrap nodes (ip:port) |
| HEARTBEAT_INTERVAL   | 10      | Interval (in seconds) for DHT updates |
| N_CTX                | 2048    | Context size for the model |
| N_BATCH              | 8       | Batch size for inference |
| N_GPU_LAYERS         | 0       | Number of layers to offload to GPU (0 = CPU only) |

### Client Configuration

```python
client = Client(
    bootstrap_nodes="node1:8001,node2:8001",  # Bootstrap DHT nodes
    model="model-name",                       # Filter by model (optional)
    min_tps=1.0,                             # Minimum tokens per second
    max_load=0.8,                            # Maximum load threshold
    dht_port=8001                            # DHT port for client
)
```

## Architecture

LlamaNet uses a **Kademlia Distributed Hash Table (DHT)** for node discovery:

1. **No Central Registry**: Nodes discover each other through the DHT network
2. **Bootstrap Process**: First node creates the network, others join via bootstrap nodes
3. **Automatic Discovery**: Clients query the DHT to find available inference nodes
4. **Fault Tolerance**: Network continues operating even if nodes leave
5. **Scalability**: Logarithmic lookup time O(log n) for node discovery

### Network Formation

```
Bootstrap Node (8001) ← Node 1 (8003) ← Node 2 (8005)
       ↑                     ↑              ↑
   Client connects      Joins DHT      Joins DHT
```

### DHT Keys

- `model:{model_name}` - Find nodes serving specific models
- `node:{node_id}` - Find specific nodes by ID  
- `all_nodes` - Discover any available nodes

## Troubleshooting

### No Nodes Available
- Ensure bootstrap nodes are running and accessible
- Check DHT_PORT is not blocked by firewall
- Verify BOOTSTRAP_NODES environment variable is set correctly

### Connection Issues
- Check that inference nodes are publishing to DHT (logs should show "Published node info")
- Verify client can reach bootstrap nodes on DHT port
- Ensure model names match between nodes and client requests

## License

MIT
