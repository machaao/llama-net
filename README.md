# LlamaNet

LlamaNet is a decentralized inference swarm for LLM models using llama.cpp. It uses Kademlia DHT for truly distributed node discovery without any central registry.

## Features

- **Decentralized DHT-based node discovery** using Kademlia protocol
- Inference nodes that serve LLM models using llama.cpp
- **Async Client Library** for easy integration with async/await support
- Automatic node selection based on load and performance
- No single point of failure - fully distributed architecture
- Docker support for easy deployment

## Requirements

- Python 3.8+
- LLM models in GGUF format
- Docker (optional, for containerized deployment)

## Installation

### From Source

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/llamanet.git
   cd llamanet
   ```

2. Install the package:
   ```bash
   pip install --editable . --use-pep517
   ```

### Using pip

```bash
pip install llamanet
```

## Running Locally

### 1. Download a Model

First, download a GGUF model file. For example:
```bash
# Create models directory
mkdir -p models

# Download a GGUF model (example - replace with actual GGUF model URL)
# wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_0.gguf -O models/model.gguf
# For now, place your .gguf model file in the models directory
```

### 2. Start Bootstrap Node

The first inference node acts as a bootstrap node for the DHT network.

**Using Command Line Arguments:**
```bash
python -m inference_node.server \
  --model-path ./models/model.gguf \
  --port 8000 \
  --dht-port 8001 \
  --node-id bootstrap-node
```

**Using Environment Variables:**
```bash
export MODEL_PATH=./models/model.gguf
export PORT=8000
export DHT_PORT=8001
export NODE_ID=bootstrap-node
export BOOTSTRAP_NODES=""

python -m inference_node.server
```

### 3. Start Additional Inference Nodes

**Using Command Line Arguments:**
```bash
python -m inference_node.server \
  --model-path ./models/model.gguf \
  --port 8002 \
  --dht-port 8003 \
  --bootstrap-nodes localhost:8001
```

**Using Environment Variables:**
```bash
export MODEL_PATH=./models/model.gguf
export PORT=8002
export DHT_PORT=8003
export BOOTSTRAP_NODES="localhost:8001"

python -m inference_node.server
```

### 4. Use the Client

```python
import asyncio
from client.api import Client

async def main():
    client = Client(
        bootstrap_nodes="localhost:8001",
        model="model"  # Use the model name (filename without extension)
    )
    
    try:
        response = await client.generate(
            prompt="What is LlamaNet?",
            max_tokens=150,
            temperature=0.7
        )
        
        if response:
            print(f"Generated by node {response.node_id}:")
            print(response.text)
        else:
            print("Failed to generate text")
    finally:
        await client.close()

if __name__ == "__main__":
    asyncio.run(main())
```

## Quick Start Commands

```bash
# 1. Install dependencies
pip install -e .

# 2. Download a model (example)
mkdir models
# Place your .gguf model file in the models directory

# 3. Start bootstrap node
python -m inference_node.server --model-path ./models/your-model.gguf

# 4. In another terminal, start additional node
python -m inference_node.server \
  --model-path ./models/your-model.gguf \
  --port 8002 \
  --dht-port 8003 \
  --bootstrap-nodes localhost:8001

# 5. Use the client (see examples/simple_client.py for full example)
# Note: client.generate() must be awaited in an async function
```

## Docker Deployment

### Using docker-compose

1. Place your model files in the `models` directory

2. Start the services:
   ```bash
   cd docker
   docker-compose up -d
   ```

This will start:
- Bootstrap node on ports 8000 (HTTP) and 8001 (DHT)
- Additional inference nodes that automatically join the DHT network

### Manual Docker Setup

1. Build the inference image:
   ```bash
   docker build -f docker/inference.Dockerfile -t llamanet-inference .
   ```

2. Start bootstrap node:
   ```bash
   docker run -d -p 8000:8000 -p 8001:8001 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e PORT=8000 \
     -e DHT_PORT=8001 \
     -e NODE_ID=bootstrap-node \
     -e BOOTSTRAP_NODES="" \
     --name llamanet-bootstrap llamanet-inference
   ```

3. Start additional nodes:
   ```bash
   docker run -d -p 8002:8000 -p 8003:8001 \
     -v /path/to/models:/models \
     -e MODEL_PATH=/models/your-model.gguf \
     -e PORT=8000 \
     -e DHT_PORT=8001 \
     -e BOOTSTRAP_NODES="host-ip:8001" \
     --name llamanet-node1 llamanet-inference
   ```

## Configuration

### Inference Node

| Environment Variable | Default | Description |
|----------------------|---------|-------------|
| MODEL_PATH           | -       | Path to the GGUF model file |
| HOST                 | 0.0.0.0 | Host to bind the inference service |
| PORT                 | 8000    | Port for the inference HTTP API |
| DHT_PORT             | 8001    | Port for Kademlia DHT protocol |
| NODE_ID              | auto    | Unique identifier for this node |
| BOOTSTRAP_NODES      | ""      | Comma-separated list of bootstrap nodes (ip:port) |
| HEARTBEAT_INTERVAL   | 10      | Interval (in seconds) for DHT updates |
| N_CTX                | 2048    | Context size for the model |
| N_BATCH              | 8       | Batch size for inference |
| N_GPU_LAYERS         | 0       | Number of layers to offload to GPU (0 = CPU only) |

### Client Configuration

```python
client = Client(
    bootstrap_nodes="node1:8001,node2:8001",  # Bootstrap DHT nodes
    model="model-name",                       # Filter by model (optional)
    min_tps=1.0,                             # Minimum tokens per second
    max_load=0.8,                            # Maximum load threshold
    dht_port=8001                            # DHT port for client
)

# Note: All client operations are async and must be awaited
response = await client.generate("Your prompt here")
await client.close()  # Always close the client when done
```

## Architecture

LlamaNet uses a **Kademlia Distributed Hash Table (DHT)** for node discovery:

1. **No Central Registry**: Nodes discover each other through the DHT network
2. **Bootstrap Process**: First node creates the network, others join via bootstrap nodes
3. **Automatic Discovery**: Clients query the DHT to find available inference nodes
4. **Fault Tolerance**: Network continues operating even if nodes leave
5. **Scalability**: Logarithmic lookup time O(log n) for node discovery

### Network Formation

```
Bootstrap Node (8001) ‚Üê Node 1 (8003) ‚Üê Node 2 (8005)
       ‚Üë                     ‚Üë              ‚Üë
   Client connects      Joins DHT      Joins DHT
```

### DHT Keys

- `model:{model_name}` - Find nodes serving specific models
- `node:{node_id}` - Find specific nodes by ID  
- `all_nodes` - Discover any available nodes

## Troubleshooting

### No Nodes Available
- Ensure bootstrap nodes are running and accessible
- Check DHT_PORT is not blocked by firewall
- Verify BOOTSTRAP_NODES environment variable is set correctly

### Connection Issues
- Check that inference nodes are publishing to DHT (logs should show "Published node info")
- Verify client can reach bootstrap nodes on DHT port
- Ensure model names match between nodes and client requests

## OpenAI-Compatible API

LlamaNet now supports OpenAI-compatible endpoints, making it a drop-in replacement for OpenAI's API in many applications.

### Supported Endpoints

- `GET /v1/models` - List available models
- `POST /v1/completions` - Text completion (compatible with OpenAI's completions API)
- `POST /v1/chat/completions` - Chat completion (compatible with OpenAI's chat API)

### Using with OpenAI Python Library

```python
import openai

# Configure to use LlamaNet
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "dummy-key"  # Not used but required by the library

# Text completion
response = openai.Completion.create(
    model="llamanet",
    prompt="What is artificial intelligence?",
    max_tokens=100,
    temperature=0.7
)
print(response.choices[0].text)

# Chat completion
response = openai.ChatCompletion.create(
    model="llamanet",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing."}
    ],
    max_tokens=150
)
print(response.choices[0].message.content)
```

### Using with curl

```bash
# List models
curl http://localhost:8000/v1/models

# Text completion
curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llamanet",
    "prompt": "What is machine learning?",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# Chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llamanet",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "max_tokens": 100
  }'
```

### Compatibility Notes

- The API is compatible with most OpenAI client libraries
- Some advanced features (like function calling) are not yet supported
- Token counting is approximate and may differ from OpenAI's implementation
- Streaming responses are not yet implemented but planned for future releases

---

## Made with ‚ù§Ô∏è using MACH-AI

This project was built with love using [MACH-AI](https://machaao.com), an AI-powered development platform that enables rapid creation of production-scale applications. MACH-AI helped accelerate the development of LlamaNet's distributed architecture, OpenAI-compatible APIs, and web interface.

**Key features developed with MACH-AI:**
- üåê Decentralized DHT-based node discovery
- ü§ñ OpenAI-compatible API endpoints
- üíª Interactive web UI with real-time updates
- üîÑ Real-time network monitoring
- üìä Performance metrics and health checks

Learn more about building AI-powered applications at [machaao.com](https://machaao.com)

## License

MIT
